{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Project specification","text":""},{"location":"#proposal","title":"Proposal:","text":"<p>For applying our knowledge in machine learning models, we are interested in studying the application of the data science field and state-of-the-art techniques within the health sciences frame. In this occasion, we want to approach the diagnosis of cardiovascular diseases, particularly, coronary artery disease (CAD) since they remain a leading cause of mortality worldwide. Thus, this project aims to leverage the \"Heart Disease\" database from the paper \"International application of a new probability algorithm for the diagnosis of coronary artery disease\" to explore and implement advanced classification algorithms for diagnosis.</p>"},{"location":"#database-specification","title":"Database specification:","text":"<p>For our purpuse we used the available data in  Heart Disease. UCI Machine Learning Repository. https://doi.org/10.24432/C52P4X. </p>"},{"location":"#variables-and-target","title":"Variables and target:","text":"<p>The database uses a total of 13 explanatory variables and a single multicategory dependent variable. These exogenous attributes are aligned with the aim of the paper in the diagnosis of CAD and have been identified as most relevant in the identification on an early stage of the illness. Additionally, we used the Cleveland default set for aour aim.</p> <p> Explanatory variables: </p> <ol> <li>age: Age of the patient</li> <li>sex: Sex of the patient (Male, Female)</li> <li>cp: Chest pain type (Categorical with 4 levels-Type 1, Type 2, Type 3 and Type 4)  [Type 1:typical anginaType 2:atypical angina Type 3:non-anginal pain Type 4:asymptomatic] <li>trestbps: Resting blood pressure-in mm Hg on admission to the hospital(Continuous)</li> <li>chol: Serum cholesterol in mg/dl (Continuous)</li> <li>fbs: Fasting blood sugar &gt; 120 mg/dl (True,False)</li> <li>restecg: Resting electrocardiographic results (N(Normal), L1(Level 1), L2(Level 2))</li> <li>thalach: Maximum heart rate achieved (Continuous)</li> <li>exang: Exercise induced angina (Yes, No)</li> <li>oldpeak: ST depression induced by exercise relative to rest (Continuous)</li> <li>slope: The slope of the peak exercise ST segment (Up, Flat, Down)</li> <li>ca: Number of major vessels (0-3) colored by flourosopy (0, 1, 2, 3)</li> <li>thal: The heart status as retrieved from Thallium test N(normal),FD(fixed defect), RD(reversible defect)</li> <p> Target: </p> <ol> <li>num: diagnosis of heart disease</li> </ol>"},{"location":"#objective","title":"Objective","text":"<p>The objectives that we are chasing with our final deliverables are:</p> <ol> <li>Data Exploration:<ol> <li>Thoroughly analyze the heart disease database to understand its structure, features, and potential challenges.</li> <li>Identify patterns, correlations, and anomalies within the dataset.</li> </ol> </li> <li> <p>Preprocessing:</p> <ol> <li>Handle missing values, outliers, and ensure data quality.</li> <li>Feature scaling, normalization, and other preprocessing techniques to prepare the dataset for model training.</li> </ol> </li> <li> <p>Algorithm Selection:</p> <ol> <li>Evaluate and compare the performance of various classification algorithms families suitable for medical diagnosis (with the possibility of running at least one model per family group):<ol> <li>Regression: Linear probability model, logistic regression, and probit regression.</li> <li>Classification trees: Decision tree, Random forest, and Gradient boosting.</li> <li>Generalized linear classifiers: Support Vector Machines (SVM).</li> </ol> </li> </ol> </li> <li>Model Training:<ol> <li>Implement selected algorithms on the dataset.</li> </ol> </li> <li>Performance Evaluation:<ol> <li>Assess the models' performance using relevant metrics (accuracy, precision, recall, F1-score).</li> <li>Utilize cross-validation to ensure robustness.</li> </ol> </li> <li>Interpretability and Explainability:<ol> <li>Prioritize models with high interpretability to enhance clinical adoption.</li> <li>Provide insights into feature importance and model decision-making processes.</li> <li>Provide a full interpretation of the final model and its possible usage into the real world.</li> </ol> </li> </ol>"},{"location":"#libraries-used","title":"Libraries used","text":"<ul> <li>os</li> <li>numpy</li> <li>pandas</li> <li>matplotlib</li> <li>seaborn</li> <li>re</li> <li>plotly</li> <li>sklearn</li> <li>ucimlrepo</li> </ul>"},{"location":"Models/","title":"Analysis","text":"<p>The next steps constitude the treatment and analysis of the data. Additionally, the establishment of the best model.</p>"},{"location":"Models/#importing-the-dataset","title":"Importing the dataset:","text":"<p>For this we use the deafult method for importing data from UCI repository.</p> <p>Importing UCI dataset<pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nheart_disease = fetch_ucirepo(id=45) \n\n# data (as pandas dataframes) \nfeatures = heart_disease.data.features \ntargets = heart_disease.data.targets \n\n# metadata \nprint(heart_disease.metadata) \n\n# variable information \nprint(heart_disease.variables) \n</code></pre> By default, the data correspond to a sample of adults from Cleveland. It is constituted by 303 observations, 13 explanatory variables, and 1 multicategorical target variable.</p>"},{"location":"Models/#pre-setting-and-handling-missing-data","title":"Pre-setting and handling missing data:","text":"<p>After exploring a bit the dataset and determining the existence of missing data we opt to do two processes. The first one was the assign the correct data types according to the values of each attribute, and the second imputing and dropping rows with missing data.</p> <p>Assigning correct data types<pre><code>#Setting of correct data types\nbase_df = base_df.astype({\"sex\": \"category\", \"cp\": \"category\", \"fbs\": \"category\", \"restecg\": \"category\",\n               \"exang\": \"category\", \"slope\": \"category\", \"thal\": \"category\", \"target\": \"category\"})\n</code></pre> Handling missing data</p> <ul> <li>For numeric data we decided to imputate the mean in order to keep the consistency of the data and not loose those registers.</li> <li>Fur categorical data, in order to avoid possible bias, we decided to drop the two registers that were NAs taking into account that putting an arbitrary value there could alter our analysis.</li> </ul> Handling missing data<pre><code>#Imputation and dropping\ndf[\"ca\"] = df[\"ca\"].fillna(df[\"ca\"].mean()) #Filling with the mean \ndf.dropna(axis=0,how=\"any\",inplace=True) #Dropping qualitative missing data\n</code></pre>"},{"location":"Models/#descriptive-statistics-and-variability-across-groups","title":"Descriptive statistics and variability across groups","text":"<p>Before running our models we decided to calculate some descriptive statistics and variability test across groups to be sure that our findings were not biased by inconsistencies. For our descriptive statistics we found an approximate normal behaviour across variables. Additionally, we tested if there was a behavioural difference between men and women, and below and above the threshold of 65 (age in which an increase in cardiovascular attacks is expected).</p> <p>Finally, we conclude this section by running a correlation matrix to understand if there was a risk of collinearity that we should handle before running our models.</p>"},{"location":"Models/#running-testing-and-comparing-models","title":"Running, testing, and comparing models.","text":"<p>To sum up, we create a function that allowed us to compare with the same sample and target variables the fit and performance of 5 models: * Regression: Linear discriminant model, logistic regression, and probit regression. * Classification trees: Random forest. * Generalized linear classifiers: Supported Vector Machine.</p> <p>This function also covered the topics of training and testing, cross validation, one-hot encoding, and scaling.</p> Creating and comparing models<pre><code>## Function\n\ndef ModelPerformance(data, response):\n    \"\"\"\n    ModelPerformance() function gets the data frame and the response variable from the user \n    and aims to fit the best model out of Linear Discriminant Model (ldm_model), Logistic Regression Model (log_model),\n    Probit Regression Model (probit_model), Random Forest Model (rf_model) and Support Vector Machines Model (svm_model).\n    First the function creates training and testing data of explanatory variables as well as the response with\n    train_test_split() function where the test_size is twenty percent of the entire data frame.\n    Using Pipeline(), the function normalizes the numeric explanatory variables with StandardScaler() while creating \n    dummy variables with OneHotEncoder() for categorical variables. Then using kFold() technique data frame is splitted \n    into five folds, therefore n_splits is 5. Pipeline() function then creates the models with the preprocesses and \n    classifier as the each model.\n\n    Later on, the training data is fit to all five models with .fit(). The function extracts  \n    information that are Cross-Validation accuracy scores with cross_val_score() and have a prediction on test data with\n    .predict(). From the test data the function extracts accuracy with accuracy_score() and using re precision, recall \n    and F1-scores of each model from the classification_report(). The function first presents the the Cross-Validation \n    accuracy scores cv_scores_(model name) and Cross-Validation standard deviations of all five models in two seperate \n    bar charts plt.bar() from pyplot. From the Cross-Validation results we can interpret how is the accuracy and fluctions\n    for the each model on training data and it can help us analyze an overfit in the results. There is also a data frame \n    provided for the Cross-Validation results which is cv_scores. Using .predict() we can predict the outcome values for \n    test data which are Y_pred_(model name).The function also provides us a radar chart from plotly to analyze which model\n    is best suiting for the the data frame with four metrics that are  accuracy, precision, recall and F1-scores. From the \n    radar chart using .add_trace() we can compare the four different aspects to pick the model for prediction. Finally, the\n    function provides a data frame for these metrics of each model to visualize each result which is model_perf.\n    \"\"\"\n    # Preprocessing\n    X = data.loc[:, data.columns[data.columns != response]] \n    Y = data[response]\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=13)\n\n    numeric_features = data.columns[data.dtypes == \"int64\"]\n    categorical_features = data.columns[(data.dtypes == \"category\") &amp; (data.columns != response)]\n\n    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder())])\n\n    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n                                               ('cat', categorical_transformer, categorical_features)])\n\n    folds = KFold(n_splits=5, shuffle=True, random_state=13)\n\n    # Linear Discriminant Model\n    ldm_model = Pipeline(steps=[('preprocessor', preprocessor), \n                            ('classifier', LinearDiscriminantAnalysis())])\n    ldm_model.fit(X_train, Y_train)\n    cv_scores_ldm = cross_val_score(ldm_model, X_train, Y_train, cv=folds, scoring='accuracy')\n    Y_pred_ldm = ldm_model.predict(X_test)\n    accuracy_ldm = round(accuracy_score(Y_test, Y_pred_ldm),3)\n    report_ldm = classification_report(Y_test, Y_pred_ldm)\n    precision_ldm = float(re.search(r'weighted avg\\s+([\\d.]+)', report_ldm).group(1))\n    recall_ldm = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_ldm).group(2))\n    f1_score_ldm = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_ldm).group(3))\n\n    # Logistic Regression Model\n    log_model = Pipeline(steps=[('preprocessor', preprocessor), \n                            ('classifier', LogisticRegression())])\n\n    log_model.fit(X_train, Y_train)\n    cv_scores_log = cross_val_score(log_model, X_train, Y_train, cv=folds, scoring='accuracy')\n    Y_pred_log = log_model.predict(X_test)\n    accuracy_log = round(accuracy_score(Y_test, Y_pred_log),3)\n    report_log = classification_report(Y_test, Y_pred_log)\n    precision_log = float(re.search(r'weighted avg\\s+([\\d.]+)', report_log).group(1))\n    recall_log = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_log).group(2))\n    f1_score_log = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_log).group(3))\n\n    # Probit Regression Model\n    probit_model = Pipeline(steps=[('preprocessor', preprocessor), \n                                ('classifier',  LogisticRegression(solver='lbfgs', max_iter=1000, random_state=13))])\n    probit_model.fit(X_train, Y_train)\n    cv_scores_probit = cross_val_score(probit_model, X_train, Y_train, cv=folds, scoring='accuracy')\n    Y_pred_probit = probit_model.predict(X_test)\n    accuracy_probit = round(accuracy_score(Y_test, Y_pred_probit),3)\n    report_probit = classification_report(Y_test, Y_pred_probit)\n    precision_probit = float(re.search(r'weighted avg\\s+([\\d.]+)', report_probit).group(1))\n    recall_probit = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_probit).group(2))\n    f1_score_probit = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_probit).group(3))\n\n\n    # Random Forest Model\n    rf_model = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('classifier', RandomForestClassifier(random_state=13))])\n    rf_model.fit(X_train, Y_train)\n    cv_scores_rf = cross_val_score(rf_model, X_train, Y_train, cv=folds, scoring='accuracy')\n    Y_pred_rf = rf_model.predict(X_test)  \n    accuracy_rf = round(accuracy_score(Y_test, Y_pred_rf),3)\n    report_rf = classification_report(Y_test, Y_pred_rf)\n    precision_rf = float(re.search(r'weighted avg\\s+([\\d.]+)', report_rf).group(1))\n    recall_rf = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_rf).group(2))\n    f1_score_rf = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_rf).group(3))\n\n    # Supported Vector Machine Model\n    svm_model = Pipeline(steps=[('preprocessor', preprocessor),\n                            ('classifier', SVC(random_state=13))])\n    svm_model.fit(X_train, Y_train)\n    cv_scores_svm = cross_val_score(svm_model, X_train, Y_train, cv=folds, scoring='accuracy')\n    Y_pred_svm = svm_model.predict(X_test)\n    accuracy_svm = round(accuracy_score(Y_test, Y_pred_svm),3)\n    report_svm = classification_report(Y_test, Y_pred_svm, zero_division=0)\n    precision_svm = float(re.search(r'weighted avg\\s+([\\d.]+)', report_svm).group(1))\n    recall_svm = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_svm).group(2))\n    f1_score_svm = float(re.search(r'weighted avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)', report_svm).group(3))\n\n    # Cross Validation mean score and standard deviation results\n\n    cv_scores = pd.DataFrame({\"CV Mean Score\":[np.mean(cv_scores_ldm),np.mean(cv_scores_log),np.mean(cv_scores_probit),\n                                               np.mean(cv_scores_rf),np.mean(cv_scores_svm)],\n                             \"CV Standard Deviation\":[np.std(cv_scores_ldm),np.std(cv_scores_log),np.std(cv_scores_probit),\n                                                 np.std(cv_scores_rf),np.std(cv_scores_svm)]})\n    cv_scores.index = [\"LDM\",\"LOG\",\"PRO\",\"RF\",\"SVM\"]\n\n    print(\"Cross Validation Results: \\n\")\n\n    plt.bar([\"LDM\",\"LOG\",\"PRO\",\"RF\",\"SVM\"], [np.mean(cv_scores_ldm),np.mean(cv_scores_log),np.mean(cv_scores_probit),\n            np.mean(cv_scores_rf),np.mean(cv_scores_svm)], color=\"forestgreen\")\n    plt.title(\"Averages of Cross Validation Scores (Accuracy)\")\n    plt.xlabel('Models')\n    plt.ylabel('Avg. CV Scores')\n    plt.show()\n\n    plt.bar([\"LDM\",\"LOG\",\"PRO\",\"RF\",\"SVM\"], [np.std(cv_scores_ldm),np.std(cv_scores_log),np.std(cv_scores_probit),\n            np.std(cv_scores_rf),np.std(cv_scores_svm)], color=\"forestgreen\")\n    plt.title(\"Standard Deviations of Cross Validation Scores (Accuracy)\")\n    plt.xlabel('Models')\n    plt.ylabel('Standard Deviations')\n    plt.show()\n\n    print(cv_scores,\"\\n\")\n\n    # Performance Analysis: Accuracy, Precision, Recall, F1-Score\n    categories = [\"Accuracy\",\"Precision\",\"Recall\",\"F1-Score\"]\n    fig = go.Figure()\n    fig.add_trace(go.Scatterpolar(\n      r=[accuracy_ldm,precision_ldm,recall_ldm,f1_score_ldm],\n      theta=categories,\n      fill='toself',\n      name='LDM'\n    ))\n    fig.add_trace(go.Scatterpolar(\n      r=[accuracy_log,precision_log,recall_log,f1_score_log],\n      theta=categories,\n      fill='toself',\n      name='LOG'\n    ))\n    fig.add_trace(go.Scatterpolar(\n      r=[accuracy_probit,precision_probit,recall_probit,f1_score_probit],\n      theta=categories,\n      fill='toself',\n      name='PRO'\n    ))\n    fig.add_trace(go.Scatterpolar(\n      r=[accuracy_rf,precision_rf,recall_rf,f1_score_rf],\n      theta=categories,\n      fill='toself',\n      name='RF'\n    ))\n    fig.add_trace(go.Scatterpolar(\n      r=[accuracy_svm,precision_svm,recall_svm,f1_score_svm],\n      theta=categories,\n      fill='toself',\n      name='SVM'\n    ))\n    fig.update_layout(\n      polar=dict(\n        radialaxis=dict(\n          visible=True,\n          range=[0, 1]\n        )),\n      showlegend=True\n    )\n    print(\"Model Performance Analysis: \\n\")\n    fig.show()\n    model_perf = pd.DataFrame({\"Accuracy\":[accuracy_ldm,accuracy_log,accuracy_probit,accuracy_rf,accuracy_svm],\n                               \"Precision\":[precision_ldm,precision_log,precision_probit,precision_rf,precision_svm],\n                               \"Recall\":[recall_ldm,recall_log,recall_probit,recall_rf,recall_svm],\n                               \"F1-Score\":[f1_score_ldm,f1_score_log,f1_score_probit,f1_score_rf,f1_score_svm]})\n    model_perf.index = [\"LDM\",\"LOG\",\"PRO\",\"RF\",\"SVM\"]\n    print(model_perf) \n\n    return \n\nModelPerformance(df, \"target\")  \nprint(ModelPerformance.__doc__)\n</code></pre>"},{"location":"Models/#result","title":"Result","text":"<p>As a result, we find that, for our exercise, the SVM model was the best one to perform in the test split. Not so ever, in the training set we notice a slightly better approach with the Random Forest, it depicted a better accuracy in average and a lower standard deviation.</p> <p>Thus, because we want to study the impact of our model in predicting the possibility of having a cancer diagnosis, we determine that the best model in this case is the SVM. Because, even though it was not the best performer in the training sample, it perform similar to the other models in terms of the average accuracy across folds and depicted a low variance.</p>"}]}